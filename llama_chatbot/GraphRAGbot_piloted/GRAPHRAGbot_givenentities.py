############################################################################################################
#  CHATBOT WITH Llama 3.3 70B FOR QUESTION ANSWERING & MANUAL ENTITIES  #
############################################################################################################

import os
import pickle
import re
import torch
import fitz
import networkx as nx
import numpy as np
import pandas as pd
import json
import igraph as ig
import leidenalg
import random
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_text_splitters import RecursiveCharacterTextSplitter
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer

# Debug flag
DEBUG = True

def debug(msg):
    if DEBUG:
        print("DEBUG:", msg)

# File paths for storing data
ENTITIES_CSV = "/work/Chatbot-in-academia/llama_chatbot/GraphRAGbot_piloted/entities.csv"
RELATIONSHIPS_CSV = "/work/Chatbot-in-academia/llama_chatbot/GraphRAGbot_piloted/relationships.csv"
GRAPH_PKL = "/work/Chatbot-in-academia/llama_chatbot/GraphRAGbot_piloted/knowledge_graph.pkl"
HIERARCHICAL_SUMMARIES_PKL = "/work/Chatbot-in-academia/llama_chatbot/GraphRAGbot_piloted/hierarchical_community_summaries.pkl"


################################################################################
# 1. Load Models
################################################################################

def load_summarization_model():
    debug("Loading summarization model...")
    summ_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.3-70B-Instruct")
    summ_model = AutoModelForCausalLM.from_pretrained(
        "meta-llama/Llama-3.3-70B-Instruct",
        torch_dtype=torch.float16,
        device_map="auto"
    )
    debug("Summarization model loaded.")
    return summ_tokenizer, summ_model


################################################################################
# 2. PDF Processing & Text Chunking
################################################################################

def process_pdf_for_rag(pdf_path, metadata_pages=2):
    """Process a PDF file to extract text chunks."""
    debug(f"Processing PDF: {pdf_path}")
    doc = fitz.Document(pdf_path)
    pages = doc.page_count

    # Extract metadata from the first few pages
    metadata_text = []
    for page_num in range(min(metadata_pages, pages)):
        page = doc.load_page(page_num)
        raw_text = page.get_text("text").strip()
        metadata_text.append(raw_text)
    full_metadata = "\n".join(metadata_text)

    # Attempt to extract title/authors/year from the metadata text
    title_pattern = re.compile(r'(?<=\n)([A-Z][^\n]{10,200})(?=\n)')
    author_pattern = re.compile(r'(?:by|authors?:)\s+(.+)', re.IGNORECASE)
    year_pattern = re.compile(r'\b(19|20)\d{2}\b')

    title_match = title_pattern.search(full_metadata)
    author_match = author_pattern.search(full_metadata)
    year_match = year_pattern.search(full_metadata)

    title = title_match.group(1).strip() if title_match else "Untitled"
    authors = author_match.group(1).strip() if author_match else "Unknown Authors"
    year = year_match.group(0) if year_match else "n.d."

    doc_citation = f"{authors} ({year}) - \"{title}\""

    # Extract main text from remaining pages
    main_text = []
    for page_num in range(metadata_pages, pages):
        page = doc.load_page(page_num)
        raw_text = page.get_text("text").strip()
        main_text.append(raw_text)

    # Split text into chunks using LangChain
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1200,
        chunk_overlap=100,
        length_function=len
    )

    meta_chunks = text_splitter.create_documents([full_metadata]) if full_metadata.strip() else []
    main_chunks = text_splitter.create_documents(["\n".join(main_text)]) if main_text else []

    # Convert each Document to just the text
    meta_chunks_text = [doc.page_content for doc in meta_chunks]
    main_chunks_text = [doc.page_content for doc in main_chunks]

    all_chunks = meta_chunks_text + main_chunks_text
    debug(f"Extracted {len(all_chunks)} chunks from PDF.")
    return doc_citation, all_chunks


def process_all_pdfs(pdf_paths_with_idx):
    """Process multiple PDFs and return a list-of-lists of chunks."""
    all_chunks = []
    doc_citations = []
    for (doc_idx, pdf_path) in pdf_paths_with_idx:
        debug(f"Processing PDF #{doc_idx}")
        citation, chunks = process_pdf_for_rag(pdf_path)
        all_chunks.append(chunks)
        doc_citations.append(citation)
    return all_chunks, doc_citations


################################################################################
# 3. Manual Knowledge Graph Creation
################################################################################

def create_knowledge_graph_from_manual_data(entities_data, relationships_data):
    """
    Create a knowledge graph from manually provided entities and relationships.
    
    entities_data: List of dicts with keys 'name','type','description','doc_idx','chunk_idx'
    relationships_data: List of dicts with keys 'source','target','description','strength','doc_idx','chunk_idx'
    """
    debug("Creating knowledge graph from manual data")

    G = nx.Graph()

    # Add nodes from entities
    for entity in entities_data:
        G.add_node(
            entity['name'],
            description=entity['description'],
            type=entity.get('type', 'CONCEPT'),
            doc_idx=entity.get('doc_idx', 0),
            chunk_idx=entity.get('chunk_idx', 0)
        )

    # Add edges from relationships
    for rel in relationships_data:
        source = rel['source']
        target = rel['target']
        if not G.has_node(source) or not G.has_node(target):
            debug(f"Warning: Node not found for relationship: {source} -> {target}")
            continue

        G.add_edge(
            source,
            target,
            description=rel['description'],
            weight=rel.get('strength', 5),
            doc_idx=rel.get('doc_idx', 0),
            chunk_idx=rel.get('chunk_idx', 0)
        )

    debug(f"Knowledge graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges")
    return G


################################################################################
# 4. Community Detection and Summarization
################################################################################

def detect_hierarchical_communities(G, max_levels=2):
    """Use Leiden to detect communities at multiple levels."""
    debug("Detecting hierarchical communities via Leiden algorithm")

    edges = list(G.edges())
    nodes = list(G.nodes())
    node_to_idx = {node: i for i, node in enumerate(nodes)}
    edge_list = [(node_to_idx[u], node_to_idx[v]) for u, v in edges]

    g = ig.Graph(n=len(nodes), edges=edge_list, directed=False)
    g.vs["name"] = nodes

    edge_weights = None
    if nx.get_edge_attributes(G, 'weight'):
        edge_weights = [G[u][v].get('weight', 1.0) for u, v in edges]
        g.es["weight"] = edge_weights

    hierarchical_partitions = {}

    # Level 0
    resolution0 = 0.5
    partition0 = leidenalg.find_partition(
        g, 
        leidenalg.RBConfigurationVertexPartition,
        resolution_parameter=resolution0,
        weights=edge_weights
    )
    modularity0 = g.modularity(partition0.membership, weights=edge_weights)
    debug(f"Level 0: {len(set(partition0.membership))} communities, modularity={modularity0:.4f}")

    level0_dict = {}
    for idx, cluster_id in enumerate(partition0.membership):
        level0_dict[nodes[idx]] = cluster_id
    hierarchical_partitions[0] = level0_dict

    # Additional levels
    resolutions = [1.0, 2.0]
    for level, resolution in enumerate(resolutions[:max_levels-1], 1):
        partition = leidenalg.find_partition(
            g, 
            leidenalg.RBConfigurationVertexPartition,
            resolution_parameter=resolution,
            weights=edge_weights
        )
        modularity = g.modularity(partition.membership, weights=edge_weights)
        num_communities = len(set(partition.membership))
        debug(f"Level {level}: {num_communities} communities, modularity={modularity:.4f}")

        level_dict = {}
        for idx, cluster_id in enumerate(partition.membership):
            parent_id = hierarchical_partitions[level-1][nodes[idx]]
            hierarchical_id = f"{parent_id}_{cluster_id}"
            level_dict[nodes[idx]] = hierarchical_id

        hierarchical_partitions[level] = level_dict

    # Build parent-child relationships
    community_hierarchy = {}
    for lvl in range(1, len(hierarchical_partitions)):
        parent_level = lvl - 1
        child_level = lvl
        for node in G.nodes():
            if node in hierarchical_partitions[child_level]:
                child_comm = hierarchical_partitions[child_level][node]
                parent_comm = hierarchical_partitions[parent_level][node]
                if child_comm not in community_hierarchy:
                    community_hierarchy[child_comm] = parent_comm

    debug(f"Built hierarchical community structure with {len(hierarchical_partitions)} levels")
    return hierarchical_partitions, community_hierarchy


def get_community_nodes(partition, community_id):
    """Return all nodes that belong to a given community_id in a partition dictionary."""
    return [node for node, comm_id in partition.items() if comm_id == community_id]


def summarize_community(G, community_nodes, all_chunks, summ_tokenizer, summ_model, max_tokens=1800):
    """Generate a summary for a single community of nodes."""
    debug(f"Summarizing community of {len(community_nodes)} nodes")
    if not community_nodes:
        return "No nodes in this community."

    subgraph = G.subgraph(community_nodes)
    community_info = []

    # Node info
    for node in community_nodes:
        node_type = G.nodes[node].get("type", "CONCEPT")
        node_desc = G.nodes[node].get("description", "No description available.")
        community_info.append(f"ENTITY: {node}\nTYPE: {node_type}\nDESCRIPTION: {node_desc}\n")

    # Edge info
    for u, v in subgraph.edges():
        edge_desc = G[u][v].get("description", f"Relationship between {u} and {v}.")
        community_info.append(f"RELATIONSHIP: {u} → {v}\nDESCRIPTION: {edge_desc}\n")

    # Possibly attach source text for these nodes (doc_idx/chunk_idx)
    relevant_chunks = set()
    for node in community_nodes:
        d_idx = G.nodes[node].get("doc_idx", 0)
        c_idx = G.nodes[node].get("chunk_idx", 0)
        if d_idx < len(all_chunks) and c_idx < len(all_chunks[d_idx]):
            relevant_chunks.add(all_chunks[d_idx][c_idx])

    for chunk in list(relevant_chunks)[:2]:
        community_info.append(f"SOURCE TEXT: {chunk}\n")

    combined_info = "\n".join(community_info)
    prompt = f"""
You are an academic knowledge synthesizer. Create a comprehensive summary of this research community
based on the entities, relationships, and source text provided.

Your summary should:
1. Identify the main research concepts and how they relate to each other
2. Explain the significance of these concepts in the academic context
3. Highlight any important methodologies, variables, or findings
4. Integrate all information into a coherent narrative

COMMUNITY INFORMATION:
{combined_info}

SUMMARY:
"""

    inputs = summ_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to("cuda")
    outputs = summ_model.generate(
        **inputs,
        max_new_tokens=1024,
        pad_token_id=summ_tokenizer.eos_token_id,
        do_sample=True,
        temperature=0.4
    )
    summary = summ_tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "SUMMARY:" in summary:
        summary = summary.split("SUMMARY:", 1)[1].strip()

    debug(f"Generated community summary with length {len(summary)} chars")
    return summary


def summarize_hierarchical_communities(G, hierarchical_partitions, community_hierarchy, all_chunks,
                                       summ_tokenizer, summ_model):
    """Build summaries for hierarchical communities at different levels."""
    debug("Building hierarchical community summaries")
    leaf_level = max(hierarchical_partitions.keys())
    level_summaries = {}

    # Summarize leaf-level communities first
    leaf_partitions = hierarchical_partitions[leaf_level]
    unique_leaf_comms = set(leaf_partitions.values())
    debug(f"Summarizing {len(unique_leaf_comms)} leaf communities at level {leaf_level}")

    leaf_summ = {}
    for comm_id in unique_leaf_comms:
        community_nodes = get_community_nodes(leaf_partitions, comm_id)
        summary = summarize_community(G, community_nodes, all_chunks, summ_tokenizer, summ_model)
        leaf_summ[comm_id] = summary
    level_summaries[leaf_level] = leaf_summ

    # Build up to higher levels
    for lvl in range(leaf_level - 1, -1, -1):
        debug(f"Summarizing communities at level {lvl}")
        partitions = hierarchical_partitions[lvl]
        unique_comms = set(partitions.values())
        level_summaries[lvl] = {}

        for comm_id in unique_comms:
            community_nodes = get_community_nodes(partitions, comm_id)
            child_comms = []
            for child_id, parent_id in community_hierarchy.items():
                if parent_id == comm_id:
                    child_comms.append(child_id)

            if child_comms:
                child_texts = []
                for c_id in child_comms:
                    # Next level is lvl+1
                    if c_id in level_summaries[lvl+1]:
                        child_summary = level_summaries[lvl+1][c_id]
                        child_texts.append(f"SUBCOMMUNITY SUMMARY: {child_summary}")

                combined_text = "\n\n".join(child_texts)
                prompt = f"""
You are an academic knowledge synthesizer. Create a comprehensive summary that integrates 
multiple subcommunity summaries into a coherent overview of a research area.

Your summary should:
1. Identify the common themes across subcommunities
2. Highlight the most important concepts, methodologies, and findings
3. Explain the relationships between different subcommunities
4. Present a unified view of this larger research area

SUBCOMMUNITY INFORMATION:
{combined_text}

INTEGRATED SUMMARY:
"""
                inputs = summ_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to("cuda")
                outputs = summ_model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    pad_token_id=summ_tokenizer.eos_token_id,
                    do_sample=True,
                    temperature=0.4
                )
                summary = summ_tokenizer.decode(outputs[0], skip_special_tokens=True)
                if "INTEGRATED SUMMARY:" in summary:
                    summary = summary.split("INTEGRATED SUMMARY:", 1)[1].strip()
            else:
                # If no child communities, summarize directly
                summary = summarize_community(G, community_nodes, all_chunks, summ_tokenizer, summ_model)

            level_summaries[lvl][comm_id] = summary

    total_summaries = sum(len(s) for s in level_summaries.values())
    debug(f"Generated {total_summaries} community summaries across {len(level_summaries)} levels")
    return level_summaries


################################################################################
# 5. Query Processing
################################################################################

def generate_answer_from_community_summary(question, community_summary, summ_tokenizer, summ_model):
    """Generate a partial answer + helpfulness score from a single community summary."""
    debug(f"Generating partial answer from community summary for question: {question}")
    prompt = f"""
You are a specialized academic research assistant. Extract information from the community summary
that directly answers the user's question.

Your response must include:
1. A detailed answer based ONLY on information in the community summary
2. A helpfulness score from 0-100 indicating how useful this information is for answering the question
   (where 0 = completely irrelevant, and 100 = perfectly answers the question)

If the community summary contains no relevant information, assign a score of 0.

If the user question is offensive, discriminatory, or contains harmful assumptions, refuse to answer and explain that such questions are not appropriate.

COMMUNITY SUMMARY:
{community_summary}

USER QUESTION:
{question}

ANSWER:

HELPFULNESS SCORE (0-100):
"""
    inputs = summ_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to("cuda")
    outputs = summ_model.generate(
        **inputs,
        max_new_tokens=1024,
        pad_token_id=summ_tokenizer.eos_token_id,
        do_sample=True,
        temperature=0.4
    )
    response = summ_tokenizer.decode(outputs[0], skip_special_tokens=True)

    answer = ""
    score = 0
    if "ANSWER:" in response:
        parts = response.split("ANSWER:", 1)[1].split("HELPFULNESS SCORE", 1)
        if len(parts) > 0:
            answer = parts[0].strip()

    if "HELPFULNESS SCORE (0-100):" in response:
        score_text = response.split("HELPFULNESS SCORE (0-100):", 1)[1].strip()
        match = re.search(r'\d+', score_text)
        if match:
            try:
                val = int(match.group(0))
                score = max(0, min(100, val))
            except:
                score = 0

    debug(f"Generated partial answer with helpfulness={score}")
    return answer, score


def combine_answers(question, scored_answers, summ_tokenizer, summ_model):
    """Combine multiple partial answers into a final coherent response, prioritizing by helpfulness score."""
    debug("Combining partial answers by helpfulness score")
    if not scored_answers:
        return "No relevant information found."

    # Sort by descending score
    sorted_answers = sorted(scored_answers, key=lambda x: x[1], reverse=True)
    # Filter out zeros
    filtered_answers = [(ans, scr) for ans, scr in sorted_answers if scr > 0]
    if not filtered_answers:
        return "No relevant information found in the dataset for this question."

    # Combine top 5
    selected_answers = []
    for ans, scr in filtered_answers[:5]:
        selected_answers.append(f"[Score: {scr}/100] {ans}")

    combined_text = "\n\n".join(selected_answers)
    prompt = f"""
You are an academic research assistant. Synthesize these partial answers into a final comprehensive answer.

Each partial answer has a helpfulness score (0-100). Focus on higher-scored answers but integrate all relevant info.

If the user question is offensive, discriminatory, or contains harmful assumptions, refuse to answer and explain that such questions are not appropriate.

USER QUESTION:
{question}

PARTIAL ANSWERS:
{combined_text}

FINAL COMPREHENSIVE ANSWER:
"""
    inputs = summ_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to("cuda")
    outputs = summ_model.generate(
        **inputs,
        max_new_tokens=1024,
        pad_token_id=summ_tokenizer.eos_token_id,
        do_sample=True,
        temperature=0.4
    )
    final_response = summ_tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "FINAL COMPREHENSIVE ANSWER:" in final_response:
        final_response = final_response.split("FINAL COMPREHENSIVE ANSWER:", 1)[1].strip()

    return final_response

def generate_enhanced_answer(question, community_summaries, relevant_chunks, summ_tokenizer, summ_model):
    """
    Generate a comprehensive answer that incorporates both community summaries
    and the directly retrieved text chunks from the original documents.
    """
    debug(f"Generating enhanced answer with {len(community_summaries)} summaries and {len(relevant_chunks)} chunks")
    
    # Prepare input for the model
    combined_summaries = "\n\n".join(community_summaries[:3])  # Limit to top 3 summaries for context length
    
    combined_chunks = ""
    for i, chunk in enumerate(relevant_chunks[:3], 1):  # Limit to top 3 chunks
        combined_chunks += f"SOURCE TEXT {i}:\n{chunk}\n\n"
    
    prompt = f"""
You are an expert academic research assistant. Create a comprehensive answer to the question
using BOTH the community summaries (high-level perspectives) and specific text chunks (detailed evidence).

If the user question is offensive, discriminatory, or contains harmful assumptions, refuse to answer and explain that such questions are not appropriate.

QUESTION: {question}

COMMUNITY SUMMARIES (high-level perspectives):
{combined_summaries}

RELEVANT TEXT CHUNKS (detailed evidence):
{combined_chunks}

Your answer should:
1. Integrate information from both community summaries and specific text chunks
2. Cite specific details from the text chunks when appropriate
3. Provide a complete, well-structured response that directly addresses the question
4. Be academically rigorous but accessible

COMPREHENSIVE ANSWER:
"""
    
    inputs = summ_tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048).to("cuda")
    outputs = summ_model.generate(
        **inputs,
        max_new_tokens=1024,
        pad_token_id=summ_tokenizer.eos_token_id,
        do_sample=True,  
        temperature=0.4   
    )
    enhanced_response = summ_tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if "COMPREHENSIVE ANSWER:" in enhanced_response:
        enhanced_response = enhanced_response.split("COMPREHENSIVE ANSWER:", 1)[1].strip()
    
    return enhanced_response

################################################################################
# 6. Chunk-Level Entity Retrieval
################################################################################

def find_chunks_for_entities(query, community_nodes, all_chunks, sent_transformer, top_k=5):
    """
    Perform chunk-level entity-based retrieval:
    1) For each chunk in all_chunks, check if it mentions any of the given community_nodes.
    2) Combine entity mention count with a query-chunk similarity to produce a final ranking.
    3) Return top_k chunk texts.
    """
    debug(f"Searching chunks for entity references + query similarity. Entities: {community_nodes}")
    entity_names = [n.lower() for n in community_nodes if n]  # Lowercase for matching

    candidate_chunks = []
    # Flatten all_chunks => each element all_chunks[i] is the list of chunk strings for doc i
    for doc_idx, doc_chunks in enumerate(all_chunks):
        for c_idx, chunk_text in enumerate(doc_chunks):
            chunk_lower = chunk_text.lower()
            mention_count = 0
            for ent in entity_names:
                if ent in chunk_lower:
                    mention_count += 1
            if mention_count > 0:
                # If chunk mentions at least 1 entity, check similarity
                query_vec = sent_transformer.encode(query)
                chunk_vec = sent_transformer.encode(chunk_text)
                sim_score = cosine_similarity([query_vec], [chunk_vec])[0][0]

                # Weighted approach: final = 0.3 * mention_count + 0.7 * sim
                final_score = 0.3 * mention_count + 0.7 * sim_score
                candidate_chunks.append((chunk_text, final_score))

    # Sort descending by final_score
    candidate_chunks.sort(key=lambda x: x[1], reverse=True)
    best = [c[0] for c in candidate_chunks[:top_k]]
    debug(f"Selected {len(best)} relevant chunks from entity-based scanning.")
    return best


################################################################################
# 7. Enhanced Query Processing
################################################################################

def process_query_enhanced(question, G, hierarchical_partitions, level_summaries, all_chunks,
                           sent_transformer, summ_tokenizer, summ_model):
    """
    Enhanced query processing that:
    1) Picks the best community level
    2) Finds top relevant communities
    3) Collects their nodes => does chunk-level entity-based retrieval
    4) Combines the top chunks with the community summaries => final answer
    """
    debug(f"Processing query with chunk-level entity retrieval: {question}")
    q_lower = question.lower()

    # 1. Determine community level
    general_patterns = ["main topics", "overview", "summary", "what is the data about",
                        "high level", "general themes", "main findings"]
    specific_patterns = ["specific", "detail", "exactly", "precisely", "tell me more about",
                         "what is the relationship between", "how does"]

    # If it matches a "general" question
    if any(p in q_lower for p in general_patterns):
        best_level = 0
    elif any(p in q_lower for p in specific_patterns):
        best_level = max(hierarchical_partitions.keys())
    else:
        if 1 in hierarchical_partitions:
            best_level = 1
        else:
            best_level = max(hierarchical_partitions.keys())

    debug(f"Selected community level {best_level} for query")

    # 2. Check for special queries about the knowledge graph
    if "how many" in q_lower and ("entities" in q_lower or "nodes" in q_lower or "concepts" in q_lower):
        return f"The knowledge graph contains {G.number_of_nodes()} entities and {G.number_of_edges()} edges."

    if "what are the main" in q_lower and ("entities" in q_lower or "concepts" in q_lower):
        # Return top 10 by centrality
        c_map = nx.degree_centrality(G)
        top_ents = sorted(c_map.items(), key=lambda x: x[1], reverse=True)[:10]
        response = "The main entities in the knowledge graph are:\n\n"
        for i, (ent, score) in enumerate(top_ents, 1):
            e_type = G.nodes[ent].get("type", "Unknown")
            response += f"{i}. {ent} (Type: {e_type})\n"
        return response

    # 3. Gather the relevant communities
    community_summaries = level_summaries.get(best_level, {})
    if not community_summaries:
        return "No community summaries available at that level."

    # Score each community by query similarity
    query_vec = sent_transformer.encode(question)
    comm_scores = []
    for comm_id, summary in community_summaries.items():
        summ_vec = sent_transformer.encode(summary)
        sim = cosine_similarity([query_vec], [summ_vec])[0][0]
        comm_scores.append((comm_id, summary, sim))

    comm_sorted = sorted(comm_scores, key=lambda x: x[2], reverse=True)
    threshold = 0.4
    top_communities = {}
    for comm_id, summary, score in comm_sorted:
        if score >= threshold or len(top_communities) < 3:
            top_communities[comm_id] = summary
        if len(top_communities) >= 5:
            break

    if not top_communities:
        return "I couldn't find specific information to answer that question in the dataset."

    # 4. For each of these communities, gather their nodes
    partitions = hierarchical_partitions[best_level]
    all_community_nodes = []
    for comm_id in top_communities:
        comm_nodes = get_community_nodes(partitions, comm_id)
        all_community_nodes.extend(comm_nodes)

    # 5. Do chunk-level entity-based retrieval
    relevant_chunks = find_chunks_for_entities(question, all_community_nodes, all_chunks, sent_transformer, top_k=5)
    if not relevant_chunks:
        return "I couldn't find relevant information in the source materials to answer that question."

    # 6. Create final answer from top communities + chunk references
    debug("Using enhanced answer generation with community summaries and text chunks")
    final_answer = generate_enhanced_answer(question, list(top_communities.values()), relevant_chunks, summ_tokenizer, summ_model)

    return final_answer

################################################################################
# Main
################################################################################

def main():
    debug("Starting GraphRAG chatbot with chunk-based entity retrieval...")

    # 1. Load models
    debug("Loading models...")
    summ_tokenizer, summ_model = load_summarization_model()
    sent_transformer = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

    # 2. Process PDFs => chunk them
    pdf_paths = ["/work/Chatbot-in-academia/papers-testing/6495.pdf"]
    pdf_paths_with_idx = [(i, path) for i, path in enumerate(pdf_paths)]
    all_chunks, doc_citations = process_all_pdfs(pdf_paths_with_idx)

    # 3. Load Entities & Relationships
    debug(f"Loading entities from {ENTITIES_CSV}")
    try:
        entities_df = pd.read_csv(ENTITIES_CSV)
        entities_data = []
        for _, row in entities_df.iterrows():
            e = {
                "name": row["name"].strip().upper(),
                "type": row["type"].strip(),
                "description": row["description"].strip()
            }
            if "doc_idx" in entities_df.columns:
                e["doc_idx"] = int(row["doc_idx"]) if not pd.isna(row["doc_idx"]) else 0
            if "chunk_idx" in entities_df.columns:
                e["chunk_idx"] = int(row["chunk_idx"]) if not pd.isna(row["chunk_idx"]) else 0
            entities_data.append(e)
        debug(f"Loaded {len(entities_data)} entities.")
    except Exception as ex:
        debug(f"Error loading entities CSV: {ex}")
        print(f"Error: Could not load entities from {ENTITIES_CSV}. Check format.")
        return

    debug(f"Loading relationships from {RELATIONSHIPS_CSV}")
    try:
        relationships_df = pd.read_csv(RELATIONSHIPS_CSV)
        relationships_data = []
        for _, row in relationships_df.iterrows():
            r = {
                "source": row["source"].strip().upper(),
                "target": row["target"].strip().upper(),
                "description": row["description"].strip()
            }
            if "strength" in relationships_df.columns:
                r["strength"] = int(row["strength"]) if not pd.isna(row["strength"]) else 5
            if "doc_idx" in relationships_df.columns:
                r["doc_idx"] = int(row["doc_idx"]) if not pd.isna(row["doc_idx"]) else 0
            if "chunk_idx" in relationships_df.columns:
                r["chunk_idx"] = int(row["chunk_idx"]) if not pd.isna(row["chunk_idx"]) else 0
            relationships_data.append(r)
        debug(f"Loaded {len(relationships_data)} relationships.")
    except Exception as ex:
        debug(f"Error loading relationships CSV: {ex}")
        print(f"Error: Could not load relationships from {RELATIONSHIPS_CSV}. Check format.")
        return

    # 4. Build knowledge graph
    G = create_knowledge_graph_from_manual_data(entities_data, relationships_data)
    if GRAPH_PKL:
        with open(GRAPH_PKL, "wb") as f:
            pickle.dump(G, f)
        debug(f"Knowledge graph saved to {GRAPH_PKL}")

    # 5. Detect communities and Summarize
    debug("Detecting hierarchical communities...")
    hierarchical_partitions, community_hierarchy = detect_hierarchical_communities(G, max_levels=2)
    for lvl, partition in hierarchical_partitions.items():
        debug(f"Level {lvl} => {len(set(partition.values()))} communities")

    if os.path.exists(HIERARCHICAL_SUMMARIES_PKL):
        debug(f"Loading summaries from {HIERARCHICAL_SUMMARIES_PKL}")
        with open(HIERARCHICAL_SUMMARIES_PKL, "rb") as f:
            level_summaries = pickle.load(f)
    else:
        debug("Generating hierarchical community summaries...")
        level_summaries = summarize_hierarchical_communities(
            G, hierarchical_partitions, community_hierarchy, all_chunks,
            summ_tokenizer, summ_model
        )
        if HIERARCHICAL_SUMMARIES_PKL:
            with open(HIERARCHICAL_SUMMARIES_PKL, "wb") as f:
                pickle.dump(level_summaries, f)
            debug(f"Saved hierarchical summaries to {HIERARCHICAL_SUMMARIES_PKL}")

    # 6. Interactive Chat
    print("\nGraphRAG Chatbot (chunk-based entity retrieval) ready! Type 'exit' to quit.")
    while True:
        query = input("\nUser: ")
        if query.lower() == "exit":
            print("Exiting.")
            break

        answer = process_query_enhanced(
            query,
            G,
            hierarchical_partitions,
            level_summaries,
            all_chunks,
            sent_transformer,
            summ_tokenizer,
            summ_model
        )
        print("\nBot:", answer)


if __name__ == "__main__":
    main()
